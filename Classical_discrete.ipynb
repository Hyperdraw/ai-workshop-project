{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8090bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conuwoo/.local/lib/python3.8/site-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n",
      "2022-07-27 14:15:18.362740: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-07-27 14:15:18.362766: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedd3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_model(num_Conv2D,num_filters,num_kernel,num_lstm_units,num_action, num_frames):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(num_action))  \n",
    "  model.add(LSTM(num_lstm_units)) # LSTM\n",
    "  model.add(Dense(num_action))  \n",
    "  model.compile(loss='mean_squared_error', optimizer='Adam') # model compiles\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ff6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement(env, model, n_games=400, epochs=16, epsilon_decrease=0.05, backsight=8, discount_factor = 0.9):\n",
    "  epsilon = 1 # epsilon allows the model to explore new actions\n",
    "  state_history = []\n",
    "  value_history = []\n",
    "  env.reset() # reset the enviornment state when starting.\n",
    "  obs = env.observation_space.sample() # array with shape (x,) with values corresponding to each observation of the game. x is the number of observation.\n",
    "\n",
    "  for game_number in range(n_games):\n",
    "    states = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    memory = [obs] * backsight # after np.sum(obs,axis=2), shape changes to 2d so reshape it into 3d and divide by (255*3) for standarlization. Then multiply backsight.\n",
    "    # before np.sum, it will be 3d but np.sum will sum every [] inside of the 3d array.\n",
    "    frames = 0\n",
    "\n",
    "    while True:\n",
    "      if random.random() > epsilon: #first state, epsilon will always bigger but because epsilon decreases incrementally during the game, model will favor optimized policy model compare to random action.\n",
    "        action = np.argmax(model.predict(np.array([memory], dtype='float16'))[0]) # LSTM outputs a single value, so choose the first index.\n",
    "      else:\n",
    "        action = env.action_space.sample() # this is random action and outputs discrete integer value i.e. 1\n",
    "      \n",
    "      obs, reward, done, _info = env.step(action) # returns observation_spcae, amount of reward, boolean value if episode has terminated, and info about state.\n",
    "      env.render()  \n",
    "      time.sleep(0.05)\n",
    "      if frames % 1 == 0: # for every 32th frames\n",
    "        #print('Game', game_number, 'frame', frames) \n",
    "        actions.append(action) # action taken gets added to the array.\n",
    "        states.append(memory) # obersvation states array gets added to the array/\n",
    "        values.append([0] * env.action_space.n) # if action_space.n = 2, then it will be in a shape of [0,0]\n",
    "        memory.pop(0) # removes the first index from memory array.\n",
    "        memory.append(obs) # and replace with current observation_space. \n",
    "\n",
    "        for i, scores in enumerate(values):\n",
    "          scores[actions[i]] += reward * discount_factor ** (len(values) - i - 1) # for example, if there 2 action space, then actions[i] will consist of (0,0),(0,1),(1,0),(1,1). it will then add reward.\n",
    "      \n",
    "      frames += 1 # frames added after every episode.\n",
    "      \n",
    "      if done:\n",
    "        print('Game', game_number, 'lasted', frames, 'frames')\n",
    "        env.reset() # resets state.\n",
    "        break\n",
    "    \n",
    "    state_history.extend(states) # observation states gets added to history.\n",
    "    value_history.extend(values) # value get added to history. \n",
    "    model.fit(np.array(states, dtype='float16'), np.array(values)) # model.fit(x,y), x is state_history and y is value_history. Thus it's using state_history to predict possible values. Learning occurs here.\n",
    "    epsilon = max(0, epsilon - epsilon_decrease) # eplison decreases until 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012cbb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conuwoo/.local/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/conuwoo/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "2022-07-27 14:15:19.317905: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-27 14:15:19.317928: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-27 14:15:19.317951: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (conuwoo-Lenovo-YOGA-920-13IKB): /proc/driver/nvidia/version does not exist\n",
      "2022-07-27 14:15:19.318132: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 14:15:19.344127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1999965000 Hz\n",
      "2022-07-27 14:15:19.345804: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23ee100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-27 14:15:19.345856: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "model = cur_model(4, 8, 4,8, env.action_space.n, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170b1498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conuwoo/.local/lib/python3.8/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 lasted 500 frames\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 29.2668\n",
      "Game 1 lasted 500 frames\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 28.5960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreinforcement\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mreinforcement\u001b[0;34m(env, model, n_games, epochs, epsilon_decrease, backsight, discount_factor)\u001b[0m\n\u001b[1;32m     22\u001b[0m obs, reward, done, _info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# returns observation_spcae, amount of reward, boolean value if episode has terminated, and info about state.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()  \n\u001b[0;32m---> 24\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# for every 32th frames\u001b[39;00m\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m#print('Game', game_number, 'frame', frames) \u001b[39;00m\n\u001b[1;32m     27\u001b[0m   actions\u001b[38;5;241m.\u001b[39mappend(action) \u001b[38;5;66;03m# action taken gets added to the array.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reinforcement(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed2957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
