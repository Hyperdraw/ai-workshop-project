{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSYNaXrozL_9"
      },
      "source": [
        "# AI Workshop Project: Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUWlvc_SzJuz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from skimage.transform import resize\n",
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx_7JAVLzDps"
      },
      "source": [
        "Sequential model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15vpcUpzDLb"
      },
      "outputs": [],
      "source": [
        "def cur_model(num_Conv2D,num_filters,num_kernel,encode_depth,num_encoded_units,units_decrease,num_lstm_units,num_frames):\n",
        "  model = Sequential()\n",
        "  for i in range(num_Conv2D): # number of convolutional layers\n",
        "    model.add(Conv2D(num_filters, kernel_size=num_kernel, data_format='channels_last', activation='relu')) # data_format makes input in shape(batch_size,height, width, channels)\n",
        "  \n",
        "  #model.add(Reshape((num_frames, -1))) # reshapes output of conv2D into 2D for initial state of LSTM\n",
        "  model.add(Flatten())\n",
        "\n",
        "  for i in range(encode_depth):\n",
        "    #model.add(Dropout(0.1))\n",
        "    model.add(Dense(num_encoded_units - units_decrease * i))\n",
        "    model.add(LeakyReLU())\n",
        "    #model.add(LSTM(num_encoded_units, return_sequences=True))\n",
        "\n",
        "  #model.add(LSTM(num_lstm_units)) # LSTM\n",
        "  #model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='mean_squared_error', optimizer=Adam()) # model compiles\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXionOnc-6WG"
      },
      "outputs": [],
      "source": [
        "def reinforcement(env, models, n_games=1, epochs=1, epsilon_decrease=0.1, backsight=2, discount_factor=0.9):\n",
        "  epsilon = 1 # epsilon allows the model to explore new actions\n",
        "  state_history = []\n",
        "  value_history = []\n",
        "  env.reset() # reset the enviornment state when starting.\n",
        "  obs = env.observation_space.sample() # array with shape (x,) with values corresponding to each observation of the game. x is the number of observation.\n",
        "\n",
        "  for game_number in range(n_games):\n",
        "    states = []\n",
        "    actions = []\n",
        "    values = []\n",
        "    memory = [resize(np.sum(obs, axis=2).reshape((210, 160, 1)).astype('float') / (255 * 3), (26, 20))] * backsight # after np.sum(obs,axis=2), shape changes to 2d so reshape it into 3d and divide by (255*3) for standarlization. Then multiply backsight.\n",
        "    # before np.sum, it will be 3d but np.sum will sum every [] inside of the 3d array.\n",
        "    frames = 0\n",
        "\n",
        "    while True:\n",
        "      if False: # random.random() > epsilon: #first state, epsilon will always bigger but because epsilon decreases incrementally during the game, model will favor optimized policy model compare to random action.\n",
        "        action = np.argmax([model.predict(np.array([memory], dtype='float16'))[0][0] for model in models])\n",
        "        # action = np.argmax(model.predict(np.array([memory], dtype='float16'))[0]) # LSTM outputs a single value, so choose the first index.\n",
        "      else:\n",
        "        action = env.action_space.sample() # this is random action and outputs discrete integer value i.e. 1\n",
        "      \n",
        "      obs, reward, done, _info = env.step(action) # returns observation_spcae, amount of reward, boolean value if episode has terminated, and info about state.\n",
        "      memory.pop(0) # removes the first index from memory array.\n",
        "      memory.append(resize(np.sum(obs, axis=2).reshape((210, 160, 1)).astype('float') / (255 * 3), (26, 20))) # and replace with current observation_space. \n",
        "      actions.append(action) # action taken gets added to the array.\n",
        "      states.append(memory) # obersvation states array gets added to the array/\n",
        "      values.append([0] * env.action_space.n) # if action_space.n = 2, then it will be in a shape of [0,0]\n",
        "\n",
        "      for i, scores in enumerate(values):\n",
        "        scores[actions[i]] += reward * discount_factor ** (len(values) - i - 1) # for example, if there 2 action space, then actions[i] will consist of (0,0),(0,1),(1,0),(1,1). it will then add reward.\n",
        "      \n",
        "      frames += 1 # frames added after every episode.\n",
        "      \n",
        "      if done:\n",
        "        print('Game', game_number, 'lasted', frames, 'frames')\n",
        "        env.reset() # resets state.\n",
        "        break\n",
        "    \n",
        "    '''state_negatives = []\n",
        "    state_positives = []\n",
        "    value_negatives = []\n",
        "    value_positives = []\n",
        "\n",
        "    for i, state in enumerate(states):\n",
        "      max_i = np.argmax(np.abs(values[i]))\n",
        "      if abs(values[i][max_i]) >= 0.05:\n",
        "        if values[i][max_i] > 0:\n",
        "          state_positives.append(state)\n",
        "          value_positives.append(values[i])\n",
        "        else:\n",
        "          state_negatives.append(state)\n",
        "          value_negatives.append(values[i])\n",
        "    \n",
        "    n_extensions = min(len(value_negatives), len(value_positives))\n",
        "    state_history.extend(state_positives[:n_extensions]) # observation states gets added to history.\n",
        "    state_history.extend(state_negatives[:n_extensions])\n",
        "    value_history.extend(value_positives[:n_extensions]) # value get added to history.\n",
        "    value_history.extend(value_negatives[:n_extensions])'''\n",
        "    state_history.extend(states)\n",
        "    value_history.extend(values)\n",
        "\n",
        "    if (game_number) % 1 == 0:\n",
        "      '''if len(value_history) > 0:\n",
        "        #value_history_np = np.array(value_history)\n",
        "        #value_history_scaled = np.exp(value_history_np) / np.repeat(np.sum(np.exp(value_history), axis=1), value_history_np.shape[1]).reshape(value_history_np.shape)\n",
        "        state_history_per_action = []\n",
        "        value_history_per_action = []\n",
        "\n",
        "        for _ in range(len(models)):\n",
        "          state_history_per_action.append([])\n",
        "          value_history_per_action.append([])\n",
        "\n",
        "        for state, value in zip(state_history, value_history):\n",
        "          action_index = np.argmax(np.abs(value))\n",
        "          state_history_per_action[action_index].append(state)\n",
        "          value_history_per_action[action_index].append([value[action_index]])\n",
        "        \n",
        "        print([len(hist) for hist in value_history_per_action])\n",
        "\n",
        "        #for i, model in enumerate(models):\n",
        "        #  model.fit(np.array(state_history_per_action[i], dtype='float16'), np.array(value_history_per_action[i]), epochs=epochs) # model.fit(x,y), x is state_history and y is value_history. Thus it's using state_history to predict possible values. Learning occurs here.\n",
        "        \n",
        "        #state_history = []\n",
        "        #value_history = []'''\n",
        "    \n",
        "    epsilon = max(0, epsilon - epsilon_decrease) # eplison decreases until 0.\n",
        "  \n",
        "  value_history_np = np.array(value_history)\n",
        "  value_history_scaled =  value_history_np #np.exp(value_history_np) / np.repeat(np.sum(np.exp(value_history), axis=1), value_history_np.shape[1]).reshape(value_history_np.shape)\n",
        "  return np.array(state_history, dtype='float16'), value_history_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmxJWWyHCOhi",
        "outputId": "eac66ec9-4b21-4e38-8230-1c166e0dea5e"
      },
      "outputs": [],
      "source": [
        "!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxOMAMuEGcuT"
      },
      "outputs": [],
      "source": [
        "!wget -nc http://www.atarimania.com/roms/Roms.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "x7sBsKYzCCGy",
        "outputId": "6912cd7a-2242-492c-925e-22730d9da748"
      },
      "outputs": [],
      "source": [
        "import patoolib\n",
        "from os import mkdir\n",
        "from os.path import exists\n",
        "from shutil import rmtree\n",
        "\n",
        "if exists('roms'):\n",
        "  rmtree('roms')\n",
        "\n",
        "mkdir('roms')\n",
        "patoolib.extract_archive(\"Roms.rar\", outdir=\"roms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm9ZCVliChLP",
        "outputId": "b71130f4-09ba-4650-f28a-d6c6a6e209b0"
      },
      "outputs": [],
      "source": [
        "!ale-import-roms roms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjLM2mi__oBP"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = gym.make(\"Pong-v4\", difficulty=0)\n",
        "models = [cur_model(0, 8, 3, 8, 500, 62, 64, 8) for _ in range(env.action_space.n)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIvZRSd8HOGh",
        "outputId": "dee75bad-3d58-40fe-990b-2a2dbcb8d97e"
      },
      "outputs": [],
      "source": [
        "np.array(env.observation_space.sample()).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxMHFJc1CshD",
        "outputId": "7425fa75-c612-40dc-cf99-aba1d1b34490"
      },
      "outputs": [],
      "source": [
        "X, Y = reinforcement(env, models, n_games=8, epochs=1, epsilon_decrease=0.01)\n",
        "#X = np.load('X.npy')\n",
        "#Y = np.load('Y.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#np.save('X.npy', X)\n",
        "#np.save('Y.npy', Y)\n",
        "state_history = X\n",
        "value_history = Y\n",
        "state_history_per_action = []\n",
        "value_history_per_action = []\n",
        "\n",
        "for _ in range(len(models)):\n",
        "    state_history_per_action.append([])\n",
        "    value_history_per_action.append([])\n",
        "\n",
        "for state, value in zip(state_history, value_history):\n",
        "    action_index = np.argmax(np.abs(value))\n",
        "    state_history_per_action[action_index].append(state)\n",
        "    value_history_per_action[action_index].append([value[action_index]])\n",
        "\n",
        "print([len(hist) for hist in value_history_per_action])\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(np.concatenate(value_history_per_action).reshape(-1, 1))\n",
        "concatenated_state_history = np.concatenate([hist for hist in state_history_per_action])\n",
        "transformer = PCA(500)\n",
        "print('Fitting transformer')\n",
        "transformer.fit(concatenated_state_history.reshape(len(concatenated_state_history), -1))\n",
        "print('Fit complete')\n",
        "\n",
        "for i in range(len(models)):\n",
        "    value_history_per_action[i] = scaler.transform(np.array(value_history_per_action[i]).reshape(-1, 1)).flatten()\n",
        "    action_state_history = np.array(state_history_per_action[i])\n",
        "    state_history_per_action[i] = transformer.transform(action_state_history.reshape(len(action_state_history), -1)).reshape(len(action_state_history), -1)\n",
        "    print('Transformed', i)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    model.fit(np.array(state_history_per_action[i], dtype='float16'), np.array(value_history_per_action[i]), epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahC40zQULMm1",
        "outputId": "d5b1d4b1-83d9-4daf-c9a9-63bb1b92fa06"
      },
      "outputs": [],
      "source": [
        "for i, model in enumerate(models):\n",
        "    model.save('model/' + str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "dump(transformer, open('transformer.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
